{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load libraries and dataset\n"
      ],
      "metadata": {
        "id": "hzReg0qwmwPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKUWd3tYqf3E"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "import torch\n",
        "from tqdm import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "import pickle\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as met"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save or load data\n",
        "data = pd.read_csv('scriptie_data.csv')\n",
        "data['underspecified'] = [1] * len(data)\n",
        "control_data = pd.read_csv('scriptie_control_data.csv')\n",
        "control_data['underspecified'] = [0] * len(control_data)\n",
        "all_data = pd.concat([data, control_data], ignore_index=True)"
      ],
      "metadata": {
        "id": "ekhKBwuOxDzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract hidden states and probe\n",
        "\n",
        "The majority of the code in this section is adapted from code by Jaap Jumelet and Jelle Zuidema used for the course Interpretability & Explanability in AI"
      ],
      "metadata": {
        "id": "n08q36Ycm1WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for loading models"
      ],
      "metadata": {
        "id": "N9Jgyy6knM2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def load_model(path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
        "    config = AutoConfig.from_pretrained(path)\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    return model, tokenizer, config"
      ],
      "metadata": {
        "id": "AOAxWwvg1dsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for creating sentence masks (not really used for our purposes)"
      ],
      "metadata": {
        "id": "aeAY0LjznOZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sen_masks(\n",
        "    input_ids, tokenizer\n",
        "):\n",
        "    \"\"\"Returns a mask for the sentence positions that have a negative/positive sentiment\"\"\"\n",
        "    all_sen_masks = []\n",
        "    all_labels = []\n",
        "\n",
        "    for tokenised_sen in tqdm_notebook(input_ids.tolist()):\n",
        "        sen_mask = []\n",
        "\n",
        "        for w_idx, (token_id) in enumerate(tokenised_sen):\n",
        "            if token_id == 0:\n",
        "                sen_mask.append(False)\n",
        "                continue\n",
        "\n",
        "            word = tokenizer.convert_ids_to_tokens(token_id).replace(\"Ä \", \"\")\n",
        "            sen_mask.append(True)\n",
        "\n",
        "        all_sen_masks.append(torch.tensor(sen_mask))\n",
        "\n",
        "    padded_masks = torch.nn.utils.rnn.pad_sequence(all_sen_masks, batch_first=True)\n",
        "\n",
        "    return padded_masks, torch.tensor(all_labels)"
      ],
      "metadata": {
        "id": "IYTH2dh01_tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for extracting hidden states"
      ],
      "metadata": {
        "id": "p6yVp-qEnSWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hidden_states(\n",
        "    input_ids, attention_mask, all_sen_masks, num_layers, batch_size=128\n",
        "):\n",
        "    token_states = {\n",
        "        layer_idx: torch.zeros(all_sen_masks.sum(), 768)\n",
        "        for layer_idx in range(num_layers)\n",
        "    }\n",
        "\n",
        "    cls_states = {\n",
        "        layer_idx: torch.zeros(all_sen_masks.size(0), 768)\n",
        "        for layer_idx in range(num_layers)\n",
        "    }\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, all_sen_masks)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=False\n",
        "    )\n",
        "\n",
        "    num_extracted = 0\n",
        "    sens_extracted = 0\n",
        "    layer_idx = 0\n",
        "\n",
        "    for batch_input_ids, batch_attention_mask, batch_sen_mask in tqdm_notebook(\n",
        "        data_loader, unit=\"batches\"\n",
        "    ):\n",
        "        with torch.no_grad():\n",
        "            all_hidden_states = model(\n",
        "                batch_input_ids,\n",
        "                attention_mask=batch_attention_mask,\n",
        "                output_hidden_states=True,\n",
        "            ).hidden_states\n",
        "        for layer_idx, hidden_states in enumerate(all_hidden_states):\n",
        "            hidden_states_subset = hidden_states[batch_sen_mask]\n",
        "            subset_size = hidden_states_subset.shape[0]\n",
        "\n",
        "            token_states[layer_idx][\n",
        "                num_extracted : num_extracted + subset_size\n",
        "            ] = hidden_states_subset.cpu()\n",
        "\n",
        "\n",
        "            cls_states[layer_idx][\n",
        "                sens_extracted : sens_extracted + batch_size\n",
        "            ] = hidden_states[:,-1].cpu() # Chance -1 to 0 for XLNet\n",
        "\n",
        "        num_extracted += subset_size\n",
        "        sens_extracted += batch_size\n",
        "\n",
        "    return token_states, cls_states"
      ],
      "metadata": {
        "id": "LCiR-S-_1qka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Key functions for creating the probing datasets\n",
        "def create_probe_data(dataset, model, tokenizer, num_layers, type_tested):\n",
        "    all_sentences, sentence_labels = dataset['text'].tolist(), dataset[type_tested].tolist()\n",
        "\n",
        "\n",
        "    encoded_corpus = tokenizer(\n",
        "        all_sentences,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    input_ids = encoded_corpus.input_ids\n",
        "    attention_mask = encoded_corpus.attention_mask\n",
        "\n",
        "    all_sen_masks, token_labels = create_sen_masks(\n",
        "        input_ids,\n",
        "        tokenizer\n",
        "    )\n",
        "\n",
        "    token_states, cls_states = extract_hidden_states(\n",
        "        input_ids, attention_mask, all_sen_masks, num_layers\n",
        "    )\n",
        "\n",
        "    return token_states, cls_states, sentence_labels"
      ],
      "metadata": {
        "id": "SvAfedqB3iuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data used for probing"
      ],
      "metadata": {
        "id": "zKm5ZrN1njH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    (\"OPT\", \"facebook/opt-125m\")\n",
        "]\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "type_tested = 'underspecified'\n",
        "data_used = all_data.sample(frac=1).reset_index(drop=True)\n",
        "train, test = train_test_split(data_used, test_size=0.2)\n",
        "train = train.reset_index(drop=True); test = test.reset_index(drop=True)\n",
        "\n",
        "for name, path in models:\n",
        "    model, tokenizer, config = load_model(path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    num_layers = config.num_hidden_layers + 1\n",
        "\n",
        "    train_X, cls_train_X, cls_train_y = create_probe_data(\n",
        "        train, model, tokenizer, num_layers, type_tested\n",
        "    )\n",
        "    test_X, cls_test_X, cls_test_y = create_probe_data(\n",
        "        test, model, tokenizer, num_layers, type_tested\n",
        "    )\n",
        "\n",
        "    data_dict[name] = {\n",
        "        \"train_X\": train_X,  # Dictionary mapping layer_idx -> tensor\n",
        "        \"test_X\": test_X,\n",
        "        \"cls_train_X\": cls_train_X,\n",
        "        \"cls_train_y\": cls_train_y,\n",
        "        \"cls_test_X\": cls_test_X,\n",
        "        \"cls_test_y\": cls_test_y,\n",
        "    }"
      ],
      "metadata": {
        "id": "U86h6fR63_sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual probing (one class)"
      ],
      "metadata": {
        "id": "Jq2d76IZnnbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as met\n",
        "\n",
        "cls_probes = {\n",
        "    model_name: {\n",
        "        layer_idx: LogisticRegression(solver=\"liblinear\", penalty=\"l2\", max_iter=10)\n",
        "        for layer_idx in range(num_layers)\n",
        "    }\n",
        "    for model_name in data_dict.keys()\n",
        "}\n",
        "\n",
        "cls_probe_results = {\n",
        "    model_name: {\n",
        "      'accuracy' : [],\n",
        "      'recall'  : [],\n",
        "      'precision' : [],\n",
        "      'f1' : []\n",
        "    } for model_name in cls_probes.keys()\n",
        "}\n",
        "\n",
        "for model_name in cls_probes.keys():\n",
        "  train_mask = (train['class'] == klasje)\n",
        "  train_index_list = train.index[train_mask]\n",
        "  test_mask = (test['class'] == klasje)\n",
        "  test_index_list = test.index[test_mask]\n",
        "  for layer_idx in range(num_layers):\n",
        "      # Skip neutral sentence and binarise sentiment classification\n",
        "      train_X, train_y = (\n",
        "          data_dict[model_name][\"cls_train_X\"][layer_idx][train_index_list],\n",
        "          np.array(data_dict[model_name][\"cls_train_y\"])[train_index_list]\n",
        "      )\n",
        "\n",
        "      cls_probes[model_name][layer_idx].fit(train_X, train_y)\n",
        "\n",
        "      test_X, test_y = (\n",
        "          data_dict[model_name][\"cls_test_X\"][layer_idx][test_index_list],\n",
        "          np.array(data_dict[model_name][\"cls_test_y\"])[test_index_list]\n",
        "      )\n",
        "\n",
        "      test_pred = cls_probes[model_name][layer_idx].predict(test_X)\n",
        "\n",
        "      test_acc = met.accuracy_score(test_y, test_pred)\n",
        "      test_precision = met.precision_score(test_y, test_pred, average='macro')\n",
        "      test_recall = met.recall_score(test_y, test_pred, average='macro')\n",
        "      test_f1 = met.f1_score(test_y, test_pred, average='macro')\n",
        "\n",
        "      print(layer_idx)\n",
        "      disp = met.ConfusionMatrixDisplay(met.confusion_matrix(test_y, test_pred),\n",
        "                                        display_labels = list(set(test_y)))\n",
        "      disp.plot()\n",
        "      plt.show()\n",
        "\n",
        "      cls_probe_results[model_name][klasje]['accuracy'].append(test_acc)\n",
        "      cls_probe_results[model_name][klasje]['precision'].append(test_precision)\n",
        "      cls_probe_results[model_name][klasje]['recall'].append(test_recall)\n",
        "      cls_probe_results[model_name][klasje]['f1'].append(test_f1)"
      ],
      "metadata": {
        "id": "L-TVoCRl_BLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual probing (multiple classes)"
      ],
      "metadata": {
        "id": "Wv-DPUf1nrFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_probes = {\n",
        "    model_name: {\n",
        "        layer_idx: LogisticRegression(solver=\"liblinear\", penalty=\"l2\", max_iter=10)\n",
        "        for layer_idx in range(num_layers)\n",
        "    }\n",
        "    for model_name in data_dict.keys()\n",
        "}\n",
        "\n",
        "cls_probe_results = {\n",
        "    model_name: {\n",
        "      klasje: {\n",
        "        'accuracy' : [],\n",
        "        'recall'  : [],\n",
        "        'precision' : [],\n",
        "        'f1' : []\n",
        "      }\n",
        "      for klasje in [1,2,3,4]\n",
        "    }\n",
        "    for model_name in data_dict.keys()\n",
        "}\n",
        "\n",
        "for model_name in cls_probes.keys():\n",
        "  for klasje in [1,2,3,4]:\n",
        "    train_mask = (train['class'] == klasje)\n",
        "    train_index_list = train.index[train_mask]\n",
        "    test_mask = (test['class'] == klasje)\n",
        "    test_index_list = test.index[test_mask]\n",
        "    for layer_idx in range(num_layers):\n",
        "        train_X, train_y = (\n",
        "            data_dict[model_name][\"cls_train_X\"][layer_idx][train_index_list],\n",
        "            np.array(data_dict[model_name][\"cls_train_y\"])[train_index_list]\n",
        "        )\n",
        "\n",
        "        cls_probes[model_name][layer_idx].fit(train_X, train_y)\n",
        "\n",
        "        test_X, test_y = (\n",
        "            data_dict[model_name][\"cls_test_X\"][layer_idx][test_index_list],\n",
        "            np.array(data_dict[model_name][\"cls_test_y\"])[test_index_list]\n",
        "        )\n",
        "\n",
        "        test_pred = cls_probes[model_name][layer_idx].predict(test_X)\n",
        "\n",
        "        test_acc = met.accuracy_score(test_y, test_pred)\n",
        "        test_precision = met.precision_score(test_y, test_pred, average='macro')\n",
        "        test_recall = met.recall_score(test_y, test_pred, average='macro')\n",
        "        test_f1 = met.f1_score(test_y, test_pred, average='macro')\n",
        "\n",
        "        print(layer_idx)\n",
        "        disp = met.ConfusionMatrixDisplay(met.confusion_matrix(test_y, test_pred),\n",
        "                                          display_labels = list(set(test_y)))\n",
        "        disp.plot()\n",
        "        plt.show()\n",
        "\n",
        "        cls_probe_results[model_name][klasje]['accuracy'].append(test_acc)\n",
        "        cls_probe_results[model_name][klasje]['precision'].append(test_precision)\n",
        "        cls_probe_results[model_name][klasje]['recall'].append(test_recall)\n",
        "        cls_probe_results[model_name][klasje]['f1'].append(test_f1)"
      ],
      "metadata": {
        "id": "G_bbXPjSntXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "dP7bNzeXoLCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get 95% confidence interval (for one class)"
      ],
      "metadata": {
        "id": "MKvNmH-0pFhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def std_lower(accuracy):\n",
        "  return proportion_confint(count=accuracy *len(test), nobs = len(test))[0]\n",
        "\n",
        "cls_probe_results_lower = {\n",
        "    model_name: {\n",
        "      'accuracy' : list(map(std_lower, cls_probe_results[model_name]['accuracy'])),\n",
        "      'recall'  : list(map(std_lower, cls_probe_results[model_name]['recall'])),\n",
        "      'precision' : list(map(std_lower, cls_probe_results[model_name]['precision'])),\n",
        "      'f1' : list(map(std_lower, cls_probe_results[model_name]['f1']))\n",
        "    }\n",
        "    for model_name in cls_probe_results.keys()\n",
        "}\n",
        "\n",
        "def std_upper(accuracy):\n",
        "  return proportion_confint(count=accuracy *len(test), nobs = len(test))[1]\n",
        "\n",
        "cls_probe_results_upper = {\n",
        "    model_name: {\n",
        "      'accuracy' : list(map(std_upper, cls_probe_results[model_name]['accuracy'])),\n",
        "      'recall'  : list(map(std_upper, cls_probe_results[model_name]['recall'])),\n",
        "      'precision' : list(map(std_upper, cls_probe_results[model_name]['precision'])),\n",
        "      'f1' : list(map(std_upper, cls_probe_results[model_name]['f1']))\n",
        "    }\n",
        "    for model_name in cls_probe_results.keys()\n",
        "}"
      ],
      "metadata": {
        "id": "xqC1bLhji64n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize (for one class)"
      ],
      "metadata": {
        "id": "zkG4hD4gpt4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(10,4))\n",
        "\n",
        "for i, model_name, results in enumerate(cls_probe_results.items()):\n",
        "  x = range(len(results['accuracy']))\n",
        "  axes[i].plot(results['accuracy'], 'o-', lw=2, markersize=3, label=model_name + \" accuracy\")\n",
        "  axes[i].fill_between(x, cls_probe_results_upper[model_name]['accuracy'], cls_probe_results_lower[model_name]['accuracy'], interpolate=True, alpha=0.5)\n",
        "  axes[i].plot(results['recall'], 'o-', lw=2, markersize=3, label=model_name + \" recall\")\n",
        "  axes[i].plot(results['precision'], 'o-', lw=2, markersize=3, label=model_name + \" precision\")\n",
        "  axes[i].plot(results['f1'], 'o-', lw=2, markersize=3, label=model_name + \" F1\")\n",
        "  axes[i].set_title(model_name + \" probe accuracy\")\n",
        "  axes[i].legend()\n",
        "  axes[i].set_ylim(0,1.0)\n",
        "  axes[i].axhline(y = 0.5, color = 'grey', linestyle = '--')\n",
        "  axes[i].set_xlabel(model_name + \" layer nr.\")\n",
        "\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"probing_class_distinction.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rhEQCLPx1Kw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get 95% confidence interval (for multiple classes)"
      ],
      "metadata": {
        "id": "QD6byaYLpzI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def std_lower(accuracy):\n",
        "  return proportion_confint(count=accuracy * len(test), nobs = len(test))[0]\n",
        "\n",
        "cls_probe_results_lower = {\n",
        "    model_name: {\n",
        "        klasje: {\n",
        "          'accuracy' : list(map(std_lower, cls_probe_results[model_name][klasje]['accuracy'])),\n",
        "          'recall'  : list(map(std_lower, cls_probe_results[model_name][klasje]['recall'])),\n",
        "          'precision' : list(map(std_lower, cls_probe_results[model_name][klasje]['precision'])),\n",
        "          'f1' : list(map(std_lower, cls_probe_results[model_name][klasje]['f1']))\n",
        "        } for klasje in [1,2,3,4]]\n",
        "    }\n",
        "    for model_name in cls_probe_results.keys()\n",
        "}\n",
        "\n",
        "def std_upper(accuracy):\n",
        "  return proportion_confint(count=accuracy *len(XLNet_test), nobs = len(XLNet_test))[1]\n",
        "\n",
        "cls_probe_results_upper = {\n",
        "    model_name: {\n",
        "        klasje: {\n",
        "          'accuracy' : list(map(std_upper, cls_probe_results[model_name][klasje]['accuracy'])),\n",
        "          'recall'  : list(map(std_upper, cls_probe_results[model_name][klasje]['recall'])),\n",
        "          'precision' : list(map(std_upper, cls_probe_results[model_name][klasje]['precision'])),\n",
        "          'f1' : list(map(std_upper, cls_probe_results[model_name][klasje]['f1']))\n",
        "        } for klasje in [1,2,3,4]\n",
        "    }\n",
        "    for model_name in cls_probe_results.keys()\n",
        "}"
      ],
      "metadata": {
        "id": "VcVI2_xYBxof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize (for multiple classes)"
      ],
      "metadata": {
        "id": "NSExlTkLrV5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, results in cls_probe_results.items():\n",
        "  fig, axes = plt.subplots(2, 2, sharey=True, figsize=(10,4))\n",
        "  for klasje, plotje in zip([1,2,3,4], [(0,0), (0,1), (1,0), (1,1)]):\n",
        "    x = range(len(results[klasje]['accuracy']))\n",
        "    axes[plotje].plot(results[klasje]['accuracy'], 'o-', lw=2, markersize=3, label=model_name + \" accuracy\")\n",
        "    axes[plotje].fill_between(x, cls_probe_results_upper[model_name][klasje]['accuracy'], cls_probe_results_lower[model_name][klasje]['accuracy'], interpolate=True, alpha=0.5)\n",
        "    axes[plotje].plot(results[klasje]['recall'], 'o-', lw=2, markersize=3, label=model_name + \" recall\")\n",
        "    axes[plotje].plot(results[klasje]['precision'], 'o-', lw=2, markersize=3, label=model_name + \" precision\")\n",
        "    axes[plotje].plot(results[klasje]['f1'], 'o-', lw=2, markersize=3, label=model_name + \" F1\")\n",
        "    axes[plotje].set_title(\"Probe accuracy for \" + klasje + \" expressions\")\n",
        "    axes[plotje].axhline(y = 0.5, color = 'grey', linestyle = '--')\n",
        "    axes[plotje].set_xlabel(model_name + \" layer nr.\")\n",
        "\n",
        "  axes[(0,0)].legend()\n",
        "  axes[(0,0)].set_ylim(0,1.0)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  fig.savefig(\"probing_\" + str(klasje) + \"_\" + model_name + \".png\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "owaeVM62_EzC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
