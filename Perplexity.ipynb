{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install libraries and load data"
      ],
      "metadata": {
        "id": "4DRXHuk9Ydkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0SercGmnVeU"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import torch\n",
        "import seaborn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.stats.multicomp as multi\n",
        "import statsmodels.api as sm\n",
        "\n",
        "!pip install lmppl\n",
        "import lmppl\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import XLNetLMHeadModel, XLNetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZZRdAD4ndun"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv('scriptie_data.csv')\n",
        "control_data = pd.read_csv('scriptie_control_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate perplexities\n",
        "\n",
        "for all the below, change the string describing the model to get the smaller version of the model"
      ],
      "metadata": {
        "id": "REs29zuMYhO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate Flan-T5 perplexity"
      ],
      "metadata": {
        "id": "Oa3WCBm1YYJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = lmppl.EncoderDecoderLM('google/flan-t5-xxl')\n",
        "def calculate_FlanT5_perplexity(sentence):\n",
        "    inputs = [\"\"]\n",
        "    outputs = [sentence]\n",
        "    ppl = scorer.get_perplexity(input_texts=inputs, output_texts=outputs)\n",
        "    return ppl[0]\n",
        "data['Flan-T5-xxl perplexity'] = data['text'].apply(calculate_FlanT5_perplexity)\n",
        "control_data['Flan-T5-xxl perplexity'] = control_data['text'].apply(calculate_FlanT5_perplexity)"
      ],
      "metadata": {
        "id": "dzc-2VxRCfCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate OPT perplexity"
      ],
      "metadata": {
        "id": "9d_VizTfYtf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = lmppl.LM('facebook/opt-13b')\n",
        "def calculate_OPT_perplexity(sentence):\n",
        "    inputs = [sentence]\n",
        "    ppl = scorer.get_perplexity(input_texts=inputs)\n",
        "    return ppl[0]\n",
        "data['OPT-13b perplexity'] = data['text'].apply(calculate_OPT_perplexity)\n",
        "control_data['OPT-13b perplexity'] = control_data['text'].apply(calculate_OPT_perplexity)"
      ],
      "metadata": {
        "id": "VbSPb4_jYoCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate XLNet perplexity"
      ],
      "metadata": {
        "id": "W9ASwnYvZG_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "XLNet_model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')\n",
        "XLNet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "def calculate_XLNet_perplexity(sentence):\n",
        "    inputs = XLNet_tokenizer.encode(sentence, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "      outputs = XLNet_model(inputs, labels=inputs)\n",
        "      loss = outputs[0]\n",
        "      perplexity = torch.exp(loss).item()\n",
        "    return perplexity\n",
        "data['XLNet-large-cased perplexity'] = data['text'].apply(calculate_XLNet_perplexity)\n",
        "control_data['XLNet-large-cased perplexity'] = control_data['text'].apply(calculate_XLNet_perplexity)"
      ],
      "metadata": {
        "id": "rd0MijxNY7NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate GPT2 perplexity"
      ],
      "metadata": {
        "id": "A4R2AecqZK0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2_model = GPT2LMHeadModel.from_pretrained('gpt2-xl', output_hidden_states=True)\n",
        "GPT2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
        "GPT2_tokenizer.padding_side = \"left\" # Very Important\n",
        "GPT2_tokenizer.pad_token = GPT2_tokenizer.eos_token\n",
        "\n",
        "def calculate_GPT2_perplexity(sentence):\n",
        "    inputs = GPT2_tokenizer.encode(sentence, return_tensors='pt')\n",
        "    outputs = GPT2_model(inputs)\n",
        "    logits = outputs.logits[:, :-1, :]\n",
        "    loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.shape[-1]), inputs[:, 1:].reshape(-1))\n",
        "    perplexity = torch.exp(loss).item()\n",
        "    return perplexity\n",
        "data['GPT2-xl perplexity'] = data['text'].apply(calculate_GPT2_perplexity)\n",
        "control_data['GPT2-xl perplexity'] = control_data['text'].apply(calculate_GPT2_perplexity)"
      ],
      "metadata": {
        "id": "EuXB_6eTZMou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# from transformers import pipeline, set_seed\n",
        "import random\n",
        "generator = pipeline('text-generation', model='facebook/opt-350m')\n",
        "set_seed(42)\n",
        "for i in range(5):\n",
        "  j = random.randint(0, len(data))\n",
        "  textje = \"Here are two sentences. A: \\\"\" + control_data['text'][j] + \"\\\" and B: \\\"\" + data['text'][j] + \"\\\". Which one of these is more semantically underspecified? Please answer by saying only 'A' or 'B'. Answer:\"\n",
        "  print(generator(textje, max_new_tokens=5, num_return_sequences=1))"
      ],
      "metadata": {
        "id": "zC2CMVN_5BYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization and significance tests"
      ],
      "metadata": {
        "id": "kdar7kboaRp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Violin plots"
      ],
      "metadata": {
        "id": "QwcI4KF4gx3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def violinplotter(statistic, subdivision):\n",
        "  temp_data = data\n",
        "  temp_data['underspecified'] = [True] * len(temp_data)\n",
        "  temp_control_data = control_data\n",
        "  temp_control_data['underspecified'] = [False] * len(temp_control_data)\n",
        "  temp = pd.concat([temp_data, temp_control_data])\n",
        "  plt.figure(figsize=(10,6))\n",
        "  ax = seaborn.violinplot(temp, x=subdivision, y=statistic, hue='underspecified', cut=0)\n",
        "  #ax.set_ylim(0,1500)\n",
        "  plt.xticks(rotation = 90)\n",
        "  plt.savefig(statistic + \"_by_\" + subdivision + '.png', bbox_inches='tight')\n",
        "violinplotter('Average word frequency', 'subclass')"
      ],
      "metadata": {
        "id": "ZJlj1xowbvc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap with significance tests"
      ],
      "metadata": {
        "id": "vGD2Qc6XgzfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather required data\n",
        "models = ['XLNet-base-cased perplexity', 'XLNet-large-cased perplexity', \"OPT-125m perplexity\", \"OPT-13b perplexity\", \"GPT2-base perplexity\", \"GPT2-xl perplexity\", \"Flan-T5-base perplexity\", \"Flan-T5-xxl perplexity\"]\n",
        "subdivision = \"class\"\n",
        "classes = data[subdivision].unique()\n",
        "classes.sort()\n",
        "\n",
        "matrix = []\n",
        "labels = []\n",
        "for model in models:\n",
        "  row_of_matrix = []\n",
        "  row_of_labels = []\n",
        "  for klasje in classes:\n",
        "    stat, p = stats.mannwhitneyu(data[data[subdivision] == klasje][model], control_data[control_data[subdivision] == klasje][model])\n",
        "    q = \">\" if data[data[subdivision] == klasje][model].mean() > control_data[control_data[subdivision] == klasje][model].mean() else \"<\"\n",
        "    row_of_matrix.append(p)\n",
        "    row_of_labels.append(q)\n",
        "  matrix.append(row_of_matrix)\n",
        "  labels.append(row_of_labels)\n",
        "\n",
        "# Plot heatmap\n",
        "df_cm = pd.DataFrame(matrix,\n",
        "                     index = [\"XLNet base\", \"XLNet large\", \"OPT 125m\", \"OPT 13b\", \"GPT2 base\", \"GPT2-xl\", \"Flan-T5 base\", \"Flan-T5 xxl\"],\n",
        "                     columns = classes\n",
        "              )\n",
        "green = seaborn.light_palette(\"seagreen\", reverse=True, as_cmap=True)\n",
        "green.set_over('tomato')\n",
        "plt.figure(figsize = (7,7))\n",
        "plt.tight_layout()\n",
        "heatmap = seaborn.heatmap(df_cm, annot=labels, fmt = '', cmap=green, vmin=0, vmax=0.05, cbar_kws={'label': 'p'})\n",
        "heatmap.set(xlabel=subdivision, ylabel='Model')\n",
        "fig = heatmap.get_figure()\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"Perplexity_signifiance\" + subdivision + \".png\")"
      ],
      "metadata": {
        "id": "LqjT_8_oIKdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['XLNet-base-cased perplexity', 'XLNet-large-cased perplexity', \"OPT-125m perplexity\", \"OPT-13b perplexity\", \"GPT2-base perplexity\", \"GPT2-xl perplexity\", \"Flan-T5-base perplexity\", \"Flan-T5-xxl perplexity\"]\n",
        "relevant = data\n",
        "relevant_control = control_data\n",
        "classes = relevant['class'].unique().tolist()\n",
        "classes.sort()\n",
        "\n",
        "matrix = []\n",
        "labels = []\n",
        "for model in models:\n",
        "  row_of_matrix = []\n",
        "  row_of_labels = []\n",
        "  for klasje in classes:\n",
        "    _, p = stats.mannwhitneyu(relevant[relevant[\"class\"] == klasje][model], relevant_control[relevant_control[\"class\"] == klasje][model])\n",
        "    q = \">\" if relevant[relevant['class'] == klasje][model].mean() > relevant_control[relevant_control['class'] == klasje][model].mean() else \"<\"\n",
        "    row_of_matrix.append(p)\n",
        "    row_of_labels.append(q)\n",
        "  matrix.append(row_of_matrix)\n",
        "  labels.append(row_of_labels)"
      ],
      "metadata": {
        "id": "VTT0lyAgJk-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression fits"
      ],
      "metadata": {
        "id": "wP2FxCfXhZ-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit logistic regression model"
      ],
      "metadata": {
        "id": "gTfdCB8nhmme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['XLNet-base-cased perplexity', 'XLNet-large-cased perplexity', \"OPT-125m perplexity\", \"OPT-13b perplexity\", \"GPT2-base perplexity\", \"GPT2-xl perplexity\", \"Flan-T5-base perplexity\", \"Flan-T5-xxl perplexity\"]\n",
        "for model in models:\n",
        "  data[model + \" verschil\"] = data[model] - control_data[model]\n",
        "  control_data[model + \" verschil\"] = control_data[model] - data[model]\n",
        "data['underspecifiedness'] = [1] * len(data)\n",
        "control_data['underspecifiedness'] = [0] * len(data)\n",
        "\n",
        "# Fit multiple models and store the summaries\n",
        "summaries = []\n",
        "#for perpl in ['XLNet-base-cased perplexity', 'XLNet-large-cased perplexity', \"OPT-125m perplexity\", \"OPT-13b perplexity\", \"GPT2-base perplexity\", \"GPT2-xl perplexity\", \"Flan-T5-base perplexity\", \"Flan-T5-xxl perplexity\"]:\n",
        "#for perpl in ['XLNet-large-cased perplexity',  \"OPT-13b perplexity\", \"GPT2-xl perplexity\", \"Flan-T5-xxl perplexity\"]:\n",
        "for perpl in [\"Flan-T5-xxl perplexity\"]:\n",
        "    cumulative = []\n",
        "    for dingetje in data['Flan-T5-xxl recognition']:\n",
        "      cumulative.append(1 if dingetje.split('-')[0] == \"Correct\" else 0)\n",
        "    data['prediction_underspecifiedness'] = cumulative\n",
        "    cumulative = []\n",
        "    for dingetje in control_data['Flan-T5-xxl recognition']:\n",
        "      cumulative.append(0 if dingetje.split('-')[0] == \"Correct\" else 1)\n",
        "    control_data['prediction_underspecifiedness'] = cumulative\n",
        "\n",
        "    all_data = pd.concat([data, control_data])\n",
        "    #all_data = all_data[all_data['class'] == 1]\n",
        "\n",
        "\n",
        "    x1 = all_data['length in words']\n",
        "    x2 = all_data['average concreteness']\n",
        "    x3 = all_data['average AoA']\n",
        "    x4 = all_data['Average word frequency']\n",
        "    x5 = all_data[perpl + \" verschil\"]\n",
        "    X = np.column_stack([x1, x2, x3, x4, x5])\n",
        "    y = all_data[\"prediction_underspecifiedness\"]\n",
        "    model = sm.Logit(y, X, formula='y~x1 + x2 + x3 + x4 + x5')\n",
        "    result = model.fit()\n",
        "    summaries.append(result.summary())\n",
        "\n",
        "# Calculate average coefficients\n",
        "average_coefs = []\n",
        "for summary in summaries:\n",
        "  coefs = summary.tables[1].data[1:]\n",
        "  coefs = [[float(text) for text in row[1:]] for row in coefs]\n",
        "  average_coefs.append(coefs)\n",
        "\n",
        "average_coefs = np.average(average_coefs, 0).tolist()"
      ],
      "metadata": {
        "id": "ClLtWqPKNgw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print data as LaTeX table"
      ],
      "metadata": {
        "id": "EIz0xC0choKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latex_table(text):\n",
        "    lines = text.split('\\n')\n",
        "    table_header = r\"\"\"\n",
        "\\begin{table}[ht]\n",
        "\\centering\n",
        "\\caption{Average regression coefficients}\n",
        "\\begin{tabular}{lcccccc}\n",
        "Variable & Coefficient & Standard Error & $z$ & $P>|z|$ & $[0.0.25$ & $0.095]$\\\\\n",
        "\\hline\n",
        "\"\"\"\n",
        "    table_footer = r\"\"\"\n",
        "\\end{tabular}\n",
        "\\end{table}\n",
        "\"\"\"\n",
        "    table_rows = []\n",
        "    for line, variable in zip(lines, [\"Sentence length\", \"Avg. concreteness\", \"Avg. age of acq.\", \"Avg. word freq.\", \"Perplexity\"]):\n",
        "        table_rows.append(variable + \" & \" + line)\n",
        "\n",
        "    latex_table = table_header + \"\\n\".join(table_rows) + table_footer\n",
        "    return latex_table\n",
        "\n",
        "text = \"\\\\\\\\\\n\".join([\" & \".join([str(round(value, 4)) for value in coefs_row]) for coefs_row in average_coefs])\n",
        "print(generate_latex_table(text))"
      ],
      "metadata": {
        "id": "dBxoZBKDIc5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Miscellaneous"
      ],
      "metadata": {
        "id": "JhSznp2VhVg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data\n",
        "data.to_csv('scriptie_data.csv', index=False)\n",
        "control_data.to_csv('scriptie_control_data.csv', index=False)"
      ],
      "metadata": {
        "id": "lZDDdDI1hYFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}