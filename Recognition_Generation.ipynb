{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load libraries and dataset\n"
      ],
      "metadata": {
        "id": "9Q3uXPN6iKzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpdSiaCFbPLv"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import statsmodels.stats.multicomp as multi\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw1KCUKwbWG8"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv('scriptie_data.csv')\n",
        "control_data = pd.read_csv('scriptie_control_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recognize underspecification\n",
        "\n",
        "for the relevant model, change the \"model\" string in the first line"
      ],
      "metadata": {
        "id": "s4_DXkAZiPkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model='google/flan-t5-xxl')\n",
        "set_seed(42)\n",
        "\n",
        "def find_prediction_of_underspecified(sentence, control_sentence):\n",
        "  control_first = random.randint(0,1)\n",
        "  first_sentence = control_sentence if control_first else sentence\n",
        "  second_sentence = sentence if control_first else control_sentence\n",
        "  textje = \"Here are two sentences. A: \\\"\" + first_sentence + \"\\\". B: \\\"\" + second_sentence + \"\\\". Which one of these is more unclear? Please respond by outputting only A or B. Answer:\"\n",
        "  answer = generator(textje, max_new_tokens=1, num_return_sequences=1)[0]['generated_text']\n",
        "  answer_is_A = answer[-1] == \"A\"; answer_is_B = answer[-1] == \"B\"\n",
        "  if control_first and answer_is_B:\n",
        "    return \"Correct-B\"\n",
        "  elif control_first and answer_is_A:\n",
        "    return \"Incorrect-A\"\n",
        "  elif control_first:\n",
        "    return \"Incorrect-Neither\"\n",
        "  elif not control_first and answer_is_A:\n",
        "    return \"Correct-A\"\n",
        "  elif not control_first and answer_is_B:\n",
        "    return \"Incorrect-B\"\n",
        "  else:\n",
        "    return \"Incorrect-Neither\"\n",
        "\n",
        "outputs = []\n",
        "for sentence, control_sentence in zip(data['text'], control_data['text']):\n",
        "  outputs.append(find_prediction_of_underspecified(sentence, control_sentence))\n",
        "data['Flan-T5 recognition'] = outputs\n",
        "control_data['Flan-T5 recognition'] = outputs"
      ],
      "metadata": {
        "id": "pMEx7qJYiZwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if the accuracy is significantly higher than chance"
      ],
      "metadata": {
        "id": "EnRxksBdkl0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "for model in [\"Flan-T5-xxl recognition\", \"GPT2-xl recognition\", \"XLNet-large-cased recognition\", \"OPT-13b recognition\"]:\n",
        "  print(model)\n",
        "  subdivision = 'class'\n",
        "  for klasje in data[subdivision].unique():\n",
        "    newdata = data[data[subdivision] == klasje]\n",
        "    accuracy = len(newdata[(newdata[model] == \"Correct-A\") | (newdata[model] == \"Correct-B\")])\n",
        "    print(klasje, \"accuracy:\", accuracy/len(newdata))\n",
        "    print(klasje, \"significance:\", stats.binom_test(accuracy, n=len(newdata), p=0.5, alternative='greater'))\n",
        "  print(\"---------------\")"
      ],
      "metadata": {
        "id": "hyDAfiyZR8bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-correct underspecification"
      ],
      "metadata": {
        "id": "rUtaaW0Rjxvr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e8I8-wT6Hao"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "def generate_reasoning(sentence):\n",
        "  textje = \"Here is a sentence: \\\"\" + sentence + \"\\\". This sentence is underspecified. Please explain why this sentence is underspecified. Answer:\"\n",
        "  inputs = tokenizer(textje, return_tensors=\"pt\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "  answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "  return answer[0]\n",
        "\n",
        "def generate_specified(sentence):\n",
        "  textje = \"Here is a sentence: \\\"\" + sentence + \"\\\". This sentence is underspecified. Please formulate a version of this sentence that is less underspecified. Answer:\"\n",
        "  inputs = tokenizer(textje, return_tensors=\"pt\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "  answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "  return answer[0]\n",
        "\n",
        "sample = data.groupby('subclass', group_keys=False).apply(lambda x: x.sample(10))\n",
        "control_sample = control_data.iloc[sample.index.values]\n",
        "for sentence, control_sentence, subclass in zip(sample['text'], control_sample['text'], sample['subclass']):\n",
        "  output = generate_reasoning(sentence)\n",
        "  print(\"When prompted with the sentence \\\"\" + sentence + \"\\\", whose specified counterpart is \\\"\" + control_sentence + \"\\\" and whose subclass is \" + subclass + \", the model's reasoning for this being underspecified is \\\"\" + output + \"\\\"\\n\")\n",
        "print(\"\\n\\n---------------\\n\\n\")\n",
        "for sentence, control_sentence, subclass in zip(sample['text'], control_sample['text'], sample['subclass']):\n",
        "  output = generate_specified(sentence)\n",
        "  print(\"When prompted with the sentence \\\"\" + sentence + \"\\\", whose specified counterpart is \\\"\" + control_sentence + \"\\\" and whose subclass is \" + subclass + \", the model's specified phrasing of this sentence is \\\"\" + output + \"\\\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}